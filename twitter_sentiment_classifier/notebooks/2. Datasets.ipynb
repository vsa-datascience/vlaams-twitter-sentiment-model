{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 302 µs\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rubenbroekx/Documents/Projects/twitter-sentiment-classifier/twitter_sentiment_classifier\n",
      "time: 1.81 ms\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "This notebook makes the data training ready. It will split the data into `training`, `validation`, and `testing` datasets as well.\n",
    "\n",
    "**TODO: Split on similarity here?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 381 ms\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from store.loader import fetch_all_tweet_data\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 446 µs\n"
     ]
    }
   ],
   "source": [
    "# Fetch all tweet data from S3 (skips automatically if this is already the case)\n",
    "fetch_all_tweet_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 51576 annotations\n",
      "time: 726 ms\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.expanduser('store/data/tweets_annotated.jsonl'), 'r') as f:\n",
    "    annotations = [json.loads(line) for line in f.readlines()]\n",
    "print(f\"Loaded in {len(annotations)} annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Similarity sorting\n",
    "\n",
    "Create a sorting within the tweets based on similarity to other tweets, using the MUSE embeddings of the previous notebook to help define similarity. \n",
    "\n",
    "A general assumption when training Deep Learning model, for which our Transformer model is no exception, is that the more unique the data samples are, the more they contribute during the model's training. Hence, the tweets with the most unique sentence embeddings are more likely to be used during training.\n",
    "\n",
    "**Note: This step may take a while (~5min). Only necessary to run if `tweets_sorted.jsonl` does not yet exists!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 515 µs\n"
     ]
    }
   ],
   "source": [
    "# Number of neighbours with which the sentence compares itself (to define similarity score)\n",
    "N_NEIGHBOURS = 10\n",
    "assert N_NEIGHBOURS < 512\n",
    "assert N_NEIGHBOURS > 5  # At least the size of N_ANNOTATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.62 s\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 47096 usable tweets\n",
      "time: 20.3 ms\n"
     ]
    }
   ],
   "source": [
    "# Use only the accepted tweets (accept==True) with agreement (agreement==True) for training\n",
    "annotations_accepted = [a for a in annotations if a['accept'] and a['agreement']]\n",
    "print(f\"Total of {len(annotations_accepted)} usable tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.84 s\n"
     ]
    }
   ],
   "source": [
    "# Encode the tweets using the MUSE embeddings (from previous notebook)\n",
    "model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "\n",
    "path_to_embedding = Path(os.path.expanduser('store/muse/'))\n",
    "if not os.path.exists(path_to_embedding): os.makedirs(path_to_embedding)\n",
    "already_created = {x.stem for x in path_to_embedding.glob(\"*.npz\")}\n",
    "\n",
    "def encode(sample):\n",
    "    \"\"\"Encode the text.\"\"\"\n",
    "    tweet_id = str(sample['id'])\n",
    "    if tweet_id in already_created: \n",
    "        return np.load(path_to_embedding / (tweet_id + '.npz'))['arr_0'][0]\n",
    "    else:\n",
    "        embeddings_ar = model(sample['text']).numpy()\n",
    "        np.savez_compressed(path_to_embedding / tweet_id, embeddings_ar, allow_pickle=True)\n",
    "        already_created.add(tweet_id)\n",
    "        return embeddings_ar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 47096/47096 [00:25<00:00, 1838.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 25.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "for sample in tqdm(annotations_accepted, desc=\"Encoding\"):\n",
    "    embeddings.append(encode(sample))\n",
    "assert len(annotations_accepted) == len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a similarity-matrix on the raw embeddings\n",
    "similarity = cosine_similarity(embeddings)\n",
    "print(\"Created similarity matrix of shape:\", similarity.shape)\n",
    "\n",
    "# Put similarity with oneself to zero\n",
    "np.fill_diagonal(similarity, 0)\n",
    "\n",
    "# Constraint the similarity matrix to be between zero and one\n",
    "similarity = np.clip(similarity, a_min=0, a_max=1)\n",
    "\n",
    "# Sort the similarity-matrix (row-wise)\n",
    "print(\"Sorting the matrix, this may take a while...\")\n",
    "similarity_sorted = np.sort(similarity, axis=1, kind='heapsort')  # O(n*log(n))\n",
    "print(\" --> Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every embedding, calculate its similarity to its N_NEIGHBOURS closest neighbours\n",
    "similarity_scores = []\n",
    "for i in tqdm(range(len(embeddings)), desc='Calculating similarity'):\n",
    "    similarity_scores.append(sum(similarity_sorted[i, -N_NEIGHBOURS:]))\n",
    "assert len(similarity_scores) == len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket the similarity scores and visualise them (fitted on N_NEIGHBOURS==10)\n",
    "counter = Counter()\n",
    "for score in similarity_scores:\n",
    "    counter[round(score, 1)] += 1\n",
    "    \n",
    "# Bar plot\n",
    "values, height = zip(*sorted(counter.items()))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(values, height=height, width=0.08, zorder=2)  # Put on top of regions\n",
    "\n",
    "# Adjust axis\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 3000)\n",
    "\n",
    "# highlight regions\n",
    "plt.axvspan(0, 3, color='green', alpha=0.2)\n",
    "plt.text(1.5, 2750, 'Highly unique', fontsize=12, horizontalalignment='center')\n",
    "\n",
    "plt.axvspan(3, 6, color='yellow', alpha=0.2)\n",
    "plt.text(4.5, 2750, 'Regular', fontsize=12, horizontalalignment='center')\n",
    "\n",
    "plt.axvspan(6, 8, color='orange', alpha=0.2)\n",
    "plt.text(7, 2750, 'Mostly similar', fontsize=12, horizontalalignment='center')\n",
    "\n",
    "plt.axvspan(8, 10, color='red', alpha=0.2)\n",
    "plt.text(9, 2750, 'Near complete duplicates', fontsize=12, horizontalalignment='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the annotations with the most unique (i.e. those with the lowest score) first\n",
    "annotations_sorted = [a for _, a in sorted(zip(similarity_scores, annotations_accepted), key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the sorted data\n",
    "with open(os.path.expanduser('store/data/tweets_sorted.jsonl'), 'w') as f:\n",
    "    f.write('\\n'.join([json.dumps(a) for a in annotations_sorted]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset split\n",
    "\n",
    "Split the dataset into `training`, `validation` and `test` sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_overview(samples):\n",
    "    \"\"\"Give an overview of the samples.\"\"\"\n",
    "    print(\"Dataset overview:\")\n",
    "    print(f\" - Total of {len(samples)} samples\")\n",
    "    \n",
    "    annotators = [s['annotator'] for s in samples]\n",
    "    print(\" - Annotators:\")\n",
    "    for a in sorted(set(annotators)):\n",
    "        print(f\"   - {a}: {annotators.count(a)}\")\n",
    "        \n",
    "    labels = [s['label'] for s in samples]\n",
    "    print(\" - Labels:\")\n",
    "    for l in sorted(set(labels)):\n",
    "        print(f\"   - {l}: {labels.count(l)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the sorted tweets\n",
    "with open(os.path.expanduser('store/data/tweets_sorted.jsonl'), 'r') as f:\n",
    "    annotations = [json.loads(line) for line in f.readlines()]\n",
    "print(f\"Loaded in {len(annotations)} annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in DataFrame\n",
    "df = pd.DataFrame(\n",
    "    annotations\n",
    ")\n",
    "\n",
    "# Use tweet-ID as index\n",
    "df.set_index('id', inplace=True)\n",
    "\n",
    "# Keep only columns relevant for training\n",
    "#  Note: All tweets_sorted are accepted (accept==True) and do agree (agreement=True)\n",
    "df = df[['text', 'label', 'annotator', 'flag']]\n",
    "\n",
    "# Remove duplicate rows\n",
    "#  Note: Same tweets (flagged) annotated by multiple annotators are kept \n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Give overview of the data\n",
    "print(f\"Total size of {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Test\n",
    "\n",
    "This part of the section creates a testing dataset.\n",
    "\n",
    "For each sentiment - this being `POSITIVE`, `NEUTRAL`, and `NEGATIVE` - we collect `SIZE_SENTIMENT` samples to create the test dataset.\n",
    "\n",
    "Select only the most reliable annotators (`TRUSTED_ANNOTATORS`) to be used to create the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The total size of the test dataset\n",
    "TEST_SIZE = 3000\n",
    "\n",
    "# Number of samples of each sentiment included in the test-set\n",
    "TEST_SIZE_SENTIMENT = TEST_SIZE // 3\n",
    "\n",
    "# Select the trusted annotators\n",
    "TRUSTED_ANNOTATORS = ['H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all samples annotated by TRUSTED_ANNOTATORS\n",
    "df_t = df[df.annotator.isin(TRUSTED_ANNOTATORS)]\n",
    "print(f\"Total of {len(df_t)} samples annotated by\", TRUSTED_ANNOTATORS)\n",
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the first TEST_SIZE_SENTIMENT tweets of each sentiment\n",
    "def sample_sentiment(sentiment) -> List[int]:\n",
    "    \"\"\"Sample TEST_SIZE_SENTIMENT annotated tweet IDs for the given sentiment.\"\"\"\n",
    "    # Only consider the samples of the correct sentiment\n",
    "    df_t_s = df_t[df_t.label == sentiment]\n",
    "    \n",
    "    # Sample tweets, don't consider order of tweets (ignore sorting bias)\n",
    "    return df_t_s.sample(n=TEST_SIZE_SENTIMENT).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "idx_positive = sample_sentiment('POSITIVE')\n",
    "idx_neutral = sample_sentiment('NEUTRAL')\n",
    "idx_negative = sample_sentiment('NEGATIVE')\n",
    "\n",
    "# Check; no overlap in IDs possible\n",
    "idx_test =  set(idx_positive) | set(idx_neutral) | set(idx_negative)\n",
    "assert len(idx_test) == 3*TEST_SIZE_SENTIMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the samples used for testing\n",
    "annotations_test = []\n",
    "added_idx = set()\n",
    "for a in annotations:\n",
    "    if a['id'] in added_idx: continue\n",
    "    if a['annotator'] != 'H': continue\n",
    "    if a['id'] not in idx_test: continue\n",
    "    added_idx.add(a['id'])\n",
    "    annotations_test.append(a)\n",
    "    \n",
    "assert len(annotations_test) == 3*TEST_SIZE_SENTIMENT\n",
    "assert len([a for a in annotations_test if a['label'] == 'POSITIVE']) == TEST_SIZE_SENTIMENT\n",
    "assert len([a for a in annotations_test if a['label'] == 'NEUTRAL']) == TEST_SIZE_SENTIMENT\n",
    "assert len([a for a in annotations_test if a['label'] == 'NEGATIVE']) == TEST_SIZE_SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give an overview of the testing dataset\n",
    "give_overview(annotations_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the test-set\n",
    "with open(os.path.expanduser('store/data/tweets_test.jsonl'), 'w') as f:\n",
    "    f.write('\\n'.join([json.dumps(a) for a in annotations_test]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Validation and training\n",
    "\n",
    "Create the validation and training set by splitting up the remaining dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the remaining samples first\n",
    "already_added = set()\n",
    "def generate_key(sample):\n",
    "    \"\"\"Generate sample key.\"\"\"\n",
    "    return f\"{sample['id']}-{sample['annotator']}-{sample['label']}\"\n",
    "\n",
    "# Check which already added to test-set\n",
    "for a in annotations_test:\n",
    "    already_added.add(generate_key(a))\n",
    "assert len(already_added) == 3*TEST_SIZE_SENTIMENT\n",
    "\n",
    "keys_remaining = set()\n",
    "annotations_remaining = []\n",
    "for a in annotations:\n",
    "    if generate_key(a) in already_added: continue  # a | b is a time consuming operation\n",
    "    if generate_key(a) in keys_remaining: continue\n",
    "    keys_remaining.add(generate_key(a))\n",
    "    annotations_remaining.append(a)\n",
    "assert len(annotations_remaining) == len(keys_remaining)\n",
    "print(\"Total samples remaining for train and validation:\", len(keys_remaining))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count labels (used to balance dataset remainder)\n",
    "n_pos = sum([s['label'] == 'POSITIVE' for s in annotations_remaining])\n",
    "print(\"Number of positive:\", n_pos)\n",
    "n_neu = sum([s['label'] == 'NEUTRAL' for s in annotations_remaining])\n",
    "print(\"Number of neutral:\", n_neu)\n",
    "n_neg = sum([s['label'] == 'NEGATIVE' for s in annotations_remaining])\n",
    "print(\"Number of negative:\", n_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforce the neutral count to be below a certain threshold\n",
    "MAX_NEUTRAL = 13_500\n",
    "\n",
    "# Lower the neutral annotations by pruning off the lowest-sorted samples\n",
    "annotations_pruned = []\n",
    "neutral_count = 0\n",
    "for a in annotations_remaining:\n",
    "    if a['label'] == 'NEUTRAL':\n",
    "        if neutral_count >= MAX_NEUTRAL: \n",
    "            keys_remaining.remove(generate_key(a))\n",
    "            continue\n",
    "        neutral_count += 1\n",
    "    annotations_pruned.append(a)\n",
    "assert len(annotations_pruned) == len(keys_remaining)\n",
    "\n",
    "# Replace the current annotations with the pruned version\n",
    "annotations_remaining = annotations_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show updated label count\n",
    "n_pos = sum([s['label'] == 'POSITIVE' for s in annotations_remaining])\n",
    "print(\"Number of positive:\", n_pos)\n",
    "n_neu = sum([s['label'] == 'NEUTRAL' for s in annotations_remaining])\n",
    "print(\"Number of neutral:\", n_neu)\n",
    "n_neg = sum([s['label'] == 'NEGATIVE' for s in annotations_remaining])\n",
    "print(\"Number of negative:\", n_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning split...: 100%|██████████| 39921/39921 [00:16<00:00, 2479.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 38723\n",
      "Total validation samples: 1198\n",
      "time: 16.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the keys in a training and validation set (use keys since shuffle==True)\n",
    "#  Use 4000 for training set and remaining for validation\n",
    "keys_train, keys_val = train_test_split(list(keys_remaining), test_size=0.03)  # Little over 1000 val samples\n",
    "assert len(set(keys_train) & set(keys_val)) == 0\n",
    "\n",
    "# Assign correct (sorted) annotations\n",
    "annotations_train = []\n",
    "annotations_val = []\n",
    "for a in tqdm(annotations_remaining, desc=\"Assigning split...\"):\n",
    "    if generate_key(a) in keys_train:\n",
    "        annotations_train.append(a)\n",
    "    elif generate_key(a) in keys_val:\n",
    "        annotations_val.append(a)\n",
    "    else:\n",
    "        raise Exception(f\"Invalid key {generate_key(a)}\")\n",
    "assert len(annotations_train) == len(keys_train)\n",
    "assert len(annotations_val) == len(keys_val)\n",
    "\n",
    "print(\"Total training samples:\", len(annotations_train))\n",
    "print(\"Total validation samples:\", len(annotations_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset overview:\n",
      " - Total of 38723 samples\n",
      " - Annotators:\n",
      "   - A: 8802\n",
      "   - H: 13174\n",
      "   - I: 5610\n",
      "   - O: 4899\n",
      "   - R: 1528\n",
      "   - S: 4710\n",
      " - Labels:\n",
      "   - NEGATIVE: 12912\n",
      "   - NEUTRAL: 13081\n",
      "   - POSITIVE: 12730\n",
      "time: 42.6 ms\n"
     ]
    }
   ],
   "source": [
    "# Give an overview of the training dataset\n",
    "give_overview(annotations_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset overview:\n",
      " - Total of 1198 samples\n",
      " - Annotators:\n",
      "   - A: 285\n",
      "   - H: 374\n",
      "   - I: 172\n",
      "   - O: 164\n",
      "   - R: 53\n",
      "   - S: 150\n",
      " - Labels:\n",
      "   - NEGATIVE: 395\n",
      "   - NEUTRAL: 419\n",
      "   - POSITIVE: 384\n",
      "time: 2.28 ms\n"
     ]
    }
   ],
   "source": [
    "# Give an overview of the validation dataset\n",
    "give_overview(annotations_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 486 ms\n"
     ]
    }
   ],
   "source": [
    "# Store the training-set\n",
    "with open(os.path.expanduser('store/data/tweets_train.jsonl'), 'w') as f:\n",
    "    f.write('\\n'.join([json.dumps(a) for a in annotations_train]) + '\\n')\n",
    "    \n",
    "# Store the validation-set\n",
    "with open(os.path.expanduser('store/data/tweets_val.jsonl'), 'w') as f:\n",
    "    f.write('\\n'.join([json.dumps(a) for a in annotations_val]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
